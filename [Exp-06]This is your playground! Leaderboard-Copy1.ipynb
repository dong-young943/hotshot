{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9ed41b",
   "metadata": {},
   "source": [
    "# 작사가 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654b0a9",
   "metadata": {},
   "source": [
    "### 라이브러리 버전을 확인해 봅니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710f21ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fbe494",
   "metadata": {},
   "source": [
    "### 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69f888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a774fb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '', 'All:', 'Resolved. resolved.', '', 'First Citizen:', 'First, you know Caius Marcius is chief enemy to the people.']\n"
     ]
    }
   ],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 파일을 읽기모드로 열고\n",
    "# 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "\n",
    "# 앞에서부터 10라인만 화면에 출력해 볼까요?\n",
    "print(raw_corpus[:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "197ef564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n",
      "Resolved. resolved.\n",
      "First, you know Caius Marcius is chief enemy to the people.\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 14: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf8b90",
   "metadata": {},
   "source": [
    "### 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e86b2b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86a4330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f58a2704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fe51a43a760>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
    "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbd49152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110    5    3    0    0\n",
      "     0]\n",
      " [   2  110    4  110    5    3    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9 3034   15    3    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c00bc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n",
      "11 : you\n",
      "12 : my\n",
      "13 : a\n",
      "14 : that\n",
      "15 : ?\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 15: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "122adfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "996f9255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c9207c",
   "metadata": {},
   "source": [
    "### 인공지능 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aba4b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6321db1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 12001), dtype=float32, numpy=\n",
       "array([[[-5.17919289e-05, -1.57385584e-04,  1.57888557e-04, ...,\n",
       "          1.23970050e-04,  1.23150414e-04, -4.68383339e-04],\n",
       "        [-2.25689946e-04, -5.14548679e-04,  4.59978823e-04, ...,\n",
       "         -2.39425153e-05,  4.84794524e-04, -3.39791732e-04],\n",
       "        [-2.66546325e-04, -5.65338822e-04,  1.65579317e-04, ...,\n",
       "          3.27236427e-04,  8.09157209e-04, -1.04933693e-04],\n",
       "        ...,\n",
       "        [-1.39966584e-03,  2.35048169e-03,  5.36329346e-04, ...,\n",
       "         -1.34233665e-03,  6.39267219e-03, -4.39528929e-04],\n",
       "        [-1.33101270e-03,  2.31794687e-03,  5.16446657e-04, ...,\n",
       "         -1.46737031e-03,  6.74115960e-03, -5.09005156e-04],\n",
       "        [-1.26978371e-03,  2.26868852e-03,  4.97616420e-04, ...,\n",
       "         -1.57891505e-03,  7.02344719e-03, -5.79636486e-04]],\n",
       "\n",
       "       [[-5.17919289e-05, -1.57385584e-04,  1.57888557e-04, ...,\n",
       "          1.23970050e-04,  1.23150414e-04, -4.68383339e-04],\n",
       "        [-2.21600800e-04, -1.78344693e-04,  5.39513348e-05, ...,\n",
       "          3.57196637e-04,  1.44669189e-04, -5.12877770e-04],\n",
       "        [ 1.23037098e-04, -6.68426859e-04, -1.06554353e-05, ...,\n",
       "          3.37560690e-04,  6.73383256e-05, -3.47903610e-04],\n",
       "        ...,\n",
       "        [-1.12071726e-03,  1.85305974e-03,  9.09126422e-04, ...,\n",
       "         -1.93872710e-03,  6.92593353e-03, -4.49706393e-04],\n",
       "        [-1.10085937e-03,  1.85733731e-03,  8.33822647e-04, ...,\n",
       "         -1.99031830e-03,  7.14688189e-03, -5.29260200e-04],\n",
       "        [-1.08402595e-03,  1.85141375e-03,  7.59564340e-04, ...,\n",
       "         -2.03049160e-03,  7.32759060e-03, -6.06062007e-04]],\n",
       "\n",
       "       [[-5.17919289e-05, -1.57385584e-04,  1.57888557e-04, ...,\n",
       "          1.23970050e-04,  1.23150414e-04, -4.68383339e-04],\n",
       "        [-3.04916030e-04, -4.27874504e-04,  5.44657269e-05, ...,\n",
       "          3.20299223e-05,  1.60024749e-04, -8.52887635e-04],\n",
       "        [-4.03356389e-04, -5.54036233e-04, -1.03282633e-04, ...,\n",
       "          1.68579601e-04, -4.26056658e-06, -5.66099712e-04],\n",
       "        ...,\n",
       "        [-1.20965391e-03,  1.31151057e-03,  4.19181888e-04, ...,\n",
       "         -4.20871685e-04,  4.41683875e-03, -2.11580103e-04],\n",
       "        [-1.27097371e-03,  1.53367664e-03,  4.95530316e-04, ...,\n",
       "         -6.49024500e-04,  5.04823821e-03, -2.34891661e-04],\n",
       "        [-1.29906950e-03,  1.69181451e-03,  5.44979237e-04, ...,\n",
       "         -8.66374583e-04,  5.59746241e-03, -2.67282827e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-5.17919289e-05, -1.57385584e-04,  1.57888557e-04, ...,\n",
       "          1.23970050e-04,  1.23150414e-04, -4.68383339e-04],\n",
       "        [-6.80368203e-06, -4.03588754e-04,  3.52129049e-04, ...,\n",
       "          2.43190414e-04,  3.89836874e-04, -7.15061149e-04],\n",
       "        [ 1.45929283e-04, -6.13417884e-04,  5.54554805e-04, ...,\n",
       "          1.19338263e-04,  4.84268938e-04, -6.92168018e-04],\n",
       "        ...,\n",
       "        [-1.04746409e-03,  1.13137264e-03,  1.04395300e-03, ...,\n",
       "         -7.27440172e-04,  4.22107708e-03, -2.45087780e-04],\n",
       "        [-1.13598956e-03,  1.31391385e-03,  9.77563555e-04, ...,\n",
       "         -9.65751591e-04,  4.88133449e-03, -2.80899083e-04],\n",
       "        [-1.18889112e-03,  1.44732092e-03,  9.12131392e-04, ...,\n",
       "         -1.17743504e-03,  5.45211695e-03, -3.25829576e-04]],\n",
       "\n",
       "       [[-5.17919289e-05, -1.57385584e-04,  1.57888557e-04, ...,\n",
       "          1.23970050e-04,  1.23150414e-04, -4.68383339e-04],\n",
       "        [-1.49252501e-05,  4.05681603e-05,  2.74259859e-04, ...,\n",
       "          2.22986728e-06,  6.51319278e-05, -4.29353735e-04],\n",
       "        [ 4.41199845e-05,  1.93160362e-04,  2.48666998e-04, ...,\n",
       "          1.89352446e-04, -1.25037777e-05, -2.87551899e-04],\n",
       "        ...,\n",
       "        [-7.80655013e-04,  1.94201397e-03,  8.89920397e-04, ...,\n",
       "         -1.74547837e-04,  3.30957188e-03, -5.49292366e-04],\n",
       "        [-8.52452940e-04,  2.09489698e-03,  8.95648322e-04, ...,\n",
       "         -4.57722635e-04,  4.07356443e-03, -5.54802304e-04],\n",
       "        [-9.06251837e-04,  2.18004268e-03,  8.80722131e-04, ...,\n",
       "         -7.19289819e-04,  4.75102430e-03, -5.67661948e-04]],\n",
       "\n",
       "       [[-5.17919289e-05, -1.57385584e-04,  1.57888557e-04, ...,\n",
       "          1.23970050e-04,  1.23150414e-04, -4.68383339e-04],\n",
       "        [-2.48036522e-04, -4.41328652e-04,  2.08249228e-04, ...,\n",
       "          1.17070806e-04,  3.87326960e-04, -6.59299025e-04],\n",
       "        [-4.78265545e-04, -7.47001846e-04,  5.54847247e-05, ...,\n",
       "          7.65623408e-05,  4.93633561e-04, -4.75252862e-04],\n",
       "        ...,\n",
       "        [-1.22944731e-03,  1.35924993e-03,  1.07685872e-03, ...,\n",
       "         -6.68819936e-04,  4.08870680e-03,  6.03807159e-04],\n",
       "        [-1.33268128e-03,  1.56182121e-03,  1.08896056e-03, ...,\n",
       "         -8.80472537e-04,  4.67162207e-03,  5.19957335e-04],\n",
       "        [-1.38950069e-03,  1.70474057e-03,  1.07820833e-03, ...,\n",
       "         -1.07292610e-03,  5.19563770e-03,  4.15929884e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a07b514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a16c6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 25s 213ms/step - loss: 3.6581\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 21s 222ms/step - loss: 2.8904\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 22s 231ms/step - loss: 2.8060\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 23s 244ms/step - loss: 2.7260\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 24s 260ms/step - loss: 2.6512\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 24s 259ms/step - loss: 2.5934\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 23s 249ms/step - loss: 2.5411\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 23s 251ms/step - loss: 2.4866\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 24s 255ms/step - loss: 2.4369\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 24s 255ms/step - loss: 2.3892\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 2.3480\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 2.3050\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 2.2650\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 2.2280\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 2.1903\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 23s 252ms/step - loss: 2.1526\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 2.1169\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 2.0818\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 2.0442\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 2.0088\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 1.9749\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 1.9407\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 24s 254ms/step - loss: 1.9060\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 1.8700\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 24s 254ms/step - loss: 1.8377\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 24s 254ms/step - loss: 1.8040\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 1.7727\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 1.7410\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 24s 254ms/step - loss: 1.7098\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 24s 253ms/step - loss: 1.6778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe4fcd5d130>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c423a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fdd594a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> fly me to the moon , and let him be a man . <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> Fly me to the moon\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b5134",
   "metadata": {},
   "source": [
    "# 회고\n",
    "#### 어려웠던 점 : 생각보다 크게 없었다.\n",
    "#### 알아낸 점 : 토크나이즈해서 문자를 분류할 수 있다.\n",
    "#### 시도한 것들 : 텍스트를 넣고 자동으로 뒤에 가사를 완성했다.\n",
    "#### if 달성하지 못했을 때, : \n",
    "#### 자기 다짐 : 이런 NLP는 재밌는 거 같다. 열심히 하자!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
